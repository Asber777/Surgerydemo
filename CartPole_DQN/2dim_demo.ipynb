{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# >> tensorboard --logdir=./tensorboard\n",
    "import json\n",
    "import gym\n",
    "import datetime\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Json_Parser:\n",
    "    def __init__(self, file_name):\n",
    "        with open(file_name) as json_file:\n",
    "            self.json_data = json.load(json_file)\n",
    "\n",
    "    def load_parser(self):\n",
    "        return self.json_data\n",
    "\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, input_dim=8):\n",
    "        super().__init__()\n",
    "        self.parser = Json_Parser(\"config.json\")\n",
    "        h = self.parser.load_parser()['agent']['hidden_unit']\n",
    "        self.fc1 = nn.Linear(input_dim, h)\n",
    "        self.fc2 = nn.Linear(h, h)\n",
    "        self.fc3 = nn.Linear(h, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acti = self.parser.load_parser()['agent']['activation']\n",
    "        if acti == \"relu\":\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        elif acti == \"sigmoid\":\n",
    "            x = torch.sigmoid(self.fc1(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        elif acti == \"softmax\":\n",
    "            x = F.softmax(self.fc1(x), dim=0)\n",
    "            x = F.softmax(self.fc2(x), dim=0)\n",
    "            x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size, keys):\n",
    "        self.memory = {}\n",
    "        for key in keys:\n",
    "            self.memory[key] = collections.deque(maxlen=memory_size)\n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def save(self, observations):\n",
    "        for i, key in enumerate(self.memory.keys()):\n",
    "            self.memory[key].append(observations[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory['x'])\n",
    "\n",
    "    def sample(self, idx):\n",
    "        sub_memory = {}\n",
    "        for key in self.memory.keys():\n",
    "            sub_memory[key] = [self.memory[key][i] for i in idx]\n",
    "\n",
    "        ss, actions, rs, ss_next, dones = sub_memory.values()\n",
    "        ss = torch.stack(ss)\n",
    "        ss_next = torch.stack(ss_next)\n",
    "        rs = np.array(rs)\n",
    "        rs = torch.from_numpy(rs).float()\n",
    "\n",
    "        return (ss, actions, rs, ss_next, dones)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "        self.parser = Json_Parser(\"config.json\")\n",
    "        self.parm = self.parser.load_parser()\n",
    "        self.method = self.parm['method']\n",
    "        self.max_step = self.parm['max_step']\n",
    "        self.discount_factor = self.parm['agent']['discount_factor']\n",
    "        self.lr = self.parm['optimizer']['learning_rate']\n",
    "        self.eps = self.parm['optimizer']['eps']\n",
    "        self.eps_max = self.eps\n",
    "        self.eps_min = self.parm['optimizer']['eps_min']\n",
    "        self.eps_mid = self.parm['optimizer']['eps_mid']\n",
    "        self.eps_anneal = self.parm['optimizer']['eps_anneal']\n",
    "        self.episode_size = self.parm['episode_size']\n",
    "        self.minibatch_size = self.parm['minibatch_size']\n",
    "        self.net_update_period = self.parm['net_update_period']\n",
    "\n",
    "        self.env = gym.make('{}'.format(self.parm['env_name']))\n",
    "        self.net = Qnet(self.parm['input_dim']*2)\n",
    "        self.target_net = Qnet(self.parm['input_dim']*2)\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.replay_memory = ReplayMemory(self.parm['memory_size'], keys=[\n",
    "                                          'x', 'a', 'r', 'x_next', 'done'])\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.net.parameters(), lr=self.lr, weight_decay=0)\n",
    "        self.loss = nn.SmoothL1Loss()\n",
    "\n",
    "        save_name = self.parm['env_name'] + '_' + self.method + '_' + self.parm['agent']['activation'] + '_' + \\\n",
    "            str(self.parm['agent']['hidden_unit']) + '_' +'in{}'.format(self.parm['input_dim']) + date_time\n",
    "        self.writer = SummaryWriter('./result/tensorboard/' + save_name)\n",
    "        self.net_save_path = './result/model/model_{}.pth'.format(save_name)\n",
    "        self.writer.add_text('config', json.dumps(self.parm))\n",
    "\n",
    "    def get_action(self, x):\n",
    "        if np.random.rand() < self.eps:\n",
    "            action = np.random.randint(2)\n",
    "        else:\n",
    "            self.net.eval()\n",
    "            q = self.net(x.view(1, -1))\n",
    "            action = np.argmax(q.detach().numpy())\n",
    "        return action\n",
    "\n",
    "    def epsilon_decaying(self):\n",
    "        if self.eps > self.eps_mid:\n",
    "            self.eps -= (self.eps_max-self.eps_mid)/self.eps_anneal\n",
    "        if self.eps < self.eps_mid and self.eps > self.eps_min:\n",
    "            self.eps -= (self.eps_mid-self.eps_min)/self.eps_anneal\n",
    "\n",
    "    def train(self, running_loss):\n",
    "        self.epsilon_decaying()\n",
    "\n",
    "        self.net.train()\n",
    "        minibatch_idx = np.random.choice(self.replay_memory.__len__(), self.minibatch_size)\n",
    "        ss, actions, rs, ss_next, dones = self.replay_memory.sample(minibatch_idx)\n",
    "        final_state_idx = np.nonzero(dones)\n",
    "\n",
    "        if self.method == \"double\":\n",
    "            with torch.no_grad():\n",
    "                self.net.eval()\n",
    "                q_next = self.net(ss_next)\n",
    "                q_next_ = self.target_net(ss_next)\n",
    "\n",
    "            self.net.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            q = self.net(ss)\n",
    "            q_next_max, q_next_argmax = torch.max(q_next, 1)\n",
    "            v_next = torch.gather(q_next_, 1, q_next_argmax.view(-1, 1)).squeeze()\n",
    "\n",
    "        if self.method == \"vanilla\":\n",
    "            with torch.no_grad():\n",
    "                q_next = self.target_net(ss_next)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            q = self.net(ss)\n",
    "            q_next_max, q_next_argmax = torch.max(q_next, 1)\n",
    "            v_next = q_next_max\n",
    "\n",
    "        v_next[final_state_idx] = 0\n",
    "        q_target = rs + self.discount_factor*v_next\n",
    "        actions = torch.tensor(actions).view(-1, 1)\n",
    "        q_relevant = torch.gather(q, 1, actions).squeeze()\n",
    "\n",
    "        loss = self.loss(q_relevant, q_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        running_loss = loss.item() if running_loss == 0 else 0.99 * \\\n",
    "            running_loss + 0.01*loss.item()\n",
    "\n",
    "        return running_loss\n",
    "\n",
    "    def run(self):\n",
    "        backprops_total = 0\n",
    "        running_loss = 0\n",
    "        latest_scores = collections.deque(maxlen=100)\n",
    "        pass_score = self.max_step - 4\n",
    "\n",
    "        s_now = self.env.reset()\n",
    "        s_now = s_now[[0,2]]\n",
    "        s_prev = s_now\n",
    "        score = 0 \n",
    "        terminal_flag = False\n",
    "\n",
    "        for episode in range(self.episode_size):\n",
    "            episode += 1\n",
    "            for step in range(self.max_step):\n",
    "                x = torch.from_numpy(np.concatenate(\n",
    "                    (s_now, s_now-s_prev))).float()\n",
    "                a = self.get_action(x)\n",
    "\n",
    "                s_next, r, done, _ = self.env.step(a)\n",
    "                s_next = s_next[[0,2]]\n",
    "                score += 1\n",
    "\n",
    "                x_next = torch.from_numpy(\n",
    "                    np.concatenate((s_next, s_next-s_now))).float()\n",
    "                self.replay_memory.save((x, a, r, x_next, done))\n",
    "\n",
    "                if done:\n",
    "                    latest_scores.append(score)\n",
    "                    score = 0\n",
    "                    s_now = self.env.reset()\n",
    "                    s_now = s_now[[0,2]]\n",
    "                    s_prev = s_now\n",
    "                else:\n",
    "                    s_prev = s_now\n",
    "                    s_now = s_next\n",
    "\n",
    "                if self.replay_memory.__len__() > self.minibatch_size:\n",
    "                    running_loss = self.train(running_loss)\n",
    "                    backprops_total += 1\n",
    "\n",
    "                self.writer.add_scalar('memory_size', self.replay_memory.__len__(), episode)\n",
    "                self.writer.add_scalar('epsilon', self.eps, episode)\n",
    "                self.writer.add_scalar('running_loss', running_loss, episode)\n",
    "                self.writer.add_scalar('avg_score', np.mean(latest_scores), episode)\n",
    "\n",
    "                if backprops_total % self.net_update_period == 0:\n",
    "                    self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "                if done and episode % 100 == 0:\n",
    "                    print(\"episode: {} | memory_size: {:5d} | eps: {:.3f} | running_loss: {:.3f} | last 100 avg score: {:3.1f}\".\n",
    "                          format(episode, self.replay_memory.__len__(), self.eps, running_loss, np.mean(latest_scores)))\n",
    "                    torch.save(self.net.state_dict(), self.net_save_path)\n",
    "\n",
    "                    if np.mean(latest_scores) > pass_score:\n",
    "                        print('Latest 100 average score: {}, pass score: {}, test is passed'.format(\n",
    "                            np.mean(latest_scores), pass_score))\n",
    "                        terminal_flag = True\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            if terminal_flag:\n",
    "                break\n",
    "        self.env.close()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 此结果使用 0 2 index的state\n",
    "agent = DQNAgent()\n",
    "agent.run()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/miniconda/envs/tftorch/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/miniconda/envs/tftorch/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode: 100 | memory_size:  2557 | eps: 0.924 | running_loss: 0.140 | last 100 avg score: 25.6\n",
      "episode: 200 | memory_size:  4811 | eps: 0.857 | running_loss: 0.283 | last 100 avg score: 22.5\n",
      "episode: 300 | memory_size:  7005 | eps: 0.791 | running_loss: 0.323 | last 100 avg score: 21.9\n",
      "episode: 400 | memory_size:  9186 | eps: 0.725 | running_loss: 0.402 | last 100 avg score: 21.8\n",
      "episode: 500 | memory_size: 12108 | eps: 0.638 | running_loss: 0.740 | last 100 avg score: 29.2\n",
      "episode: 600 | memory_size: 15338 | eps: 0.541 | running_loss: 1.045 | last 100 avg score: 32.3\n",
      "episode: 700 | memory_size: 19073 | eps: 0.429 | running_loss: 1.289 | last 100 avg score: 37.4\n",
      "episode: 800 | memory_size: 25134 | eps: 0.247 | running_loss: 1.375 | last 100 avg score: 60.7\n",
      "episode: 900 | memory_size: 35137 | eps: 0.085 | running_loss: 1.611 | last 100 avg score: 100.0\n",
      "episode: 1000 | memory_size: 47578 | eps: 0.047 | running_loss: 1.454 | last 100 avg score: 138.3\n",
      "episode: 1200 | memory_size: 50000 | eps: 0.010 | running_loss: 0.674 | last 100 avg score: 165.8\n",
      "episode: 1300 | memory_size: 50000 | eps: 0.010 | running_loss: 0.651 | last 100 avg score: 135.5\n",
      "episode: 1400 | memory_size: 50000 | eps: 0.010 | running_loss: 0.532 | last 100 avg score: 134.8\n",
      "episode: 1500 | memory_size: 50000 | eps: 0.010 | running_loss: 0.507 | last 100 avg score: 161.1\n",
      "episode: 1600 | memory_size: 50000 | eps: 0.010 | running_loss: 0.687 | last 100 avg score: 141.9\n",
      "episode: 1700 | memory_size: 50000 | eps: 0.010 | running_loss: 0.720 | last 100 avg score: 125.5\n",
      "episode: 1800 | memory_size: 50000 | eps: 0.010 | running_loss: 0.868 | last 100 avg score: 138.7\n",
      "episode: 1900 | memory_size: 50000 | eps: 0.010 | running_loss: 0.864 | last 100 avg score: 139.4\n",
      "episode: 2000 | memory_size: 50000 | eps: 0.010 | running_loss: 0.697 | last 100 avg score: 166.6\n",
      "episode: 2400 | memory_size: 50000 | eps: 0.010 | running_loss: 0.564 | last 100 avg score: 198.6\n",
      "Latest 100 average score: 198.58, pass score: 195, test is passed\n",
      "episode: 3000 | memory_size: 50000 | eps: 0.010 | running_loss: 0.720 | last 100 avg score: 172.7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_surgery(agent, sample_n, store_sample = False):\n",
    "    agent.env = gym.make('{}'.format(agent.parm['env_name']))\n",
    "    s_now = agent.env.reset()\n",
    "    s_now = s_now[[0,2]]\n",
    "    s_prev = s_now\n",
    "    score = 0\n",
    "    for episode in range(sample_n):\n",
    "        for step in range(agent.max_step):\n",
    "            x = torch.from_numpy(np.concatenate(\n",
    "                (s_now, s_now-s_prev))).float()\n",
    "            a = agent.get_action(x)\n",
    "            s_next, r, done, _ = agent.env.step(a)\n",
    "            s_next = s_next[[0,2]]\n",
    "            score += 1\n",
    "            x_next = torch.from_numpy(\n",
    "                np.concatenate((s_next, s_next-s_now))).float()\n",
    "            if store_sample:\n",
    "                agent.replay_memory.save((x, a, r, x_next, done))\n",
    "            if done:\n",
    "                print(\"episode: {} | score: {:3.1f}\".format(episode, score))\n",
    "                score = 0\n",
    "                s_now = agent.env.reset()\n",
    "                s_now = s_now[[0,2]]\n",
    "                s_prev = s_now\n",
    "                break\n",
    "            else:\n",
    "                s_prev = s_now\n",
    "                s_now = s_next\n",
    "    agent.env.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Kernel is dead",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:52:852667)",
      "at g.sendShellMessage (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:52:852436)",
      "at g.requestExecute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:52:854978)",
      "at d.requestExecute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:37:304068)",
      "at w.requestExecute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:24:122363)",
      "at w.executeCodeCell (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:326965)",
      "at w.execute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:326520)",
      "at w.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:322336)",
      "at async t.CellExecutionQueue.executeQueuedCells (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:336863)",
      "at async t.CellExecutionQueue.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.1236758218/out/client/extension.js:90:336403)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('tftorch': conda)"
  },
  "interpreter": {
   "hash": "3242c50314a9a68f934114b03bfda27d4d4439c8399dcbb46cdc3239617c61c1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}